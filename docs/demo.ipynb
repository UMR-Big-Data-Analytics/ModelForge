{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from uuid import uuid4\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from distributed import LocalCluster\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from modelforge.experiments.pipeline_builders.pipeline_factory import PipelineFactory\n",
    "from modelforge.experiments.pipeline_builders.prediction_loss_set_pipeline_builder import \\\n",
    "    PredictionLossSetPipelineBuilder\n",
    "from modelforge.model_clustering.consolidation.retraining_consolidation_strategy import RetrainingConsolidationStrategy\n",
    "from modelforge.model_clustering.entity.model_dataset import ModelDataSet\n",
    "from modelforge.model_clustering.entity.model_entity import ModelEntity\n",
    "from modelforge.model_clustering.evaluator.model_entity_data_merger import PandasModelEntityDataMerger\n",
    "from modelforge.model_clustering.transformer.sampler.set.criterion_selector.target_selector import TargetSelector\n",
    "from modelforge.model_clustering.transformer.sampler.set.measure.entropy import Entropy\n",
    "from modelforge.model_clustering.transformer.sampler.set.uniform_set_sampler import UniformSetSampler\n"
   ],
   "id": "a4500c91f9982771"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Wrap a regular sklearn model as kwargs are not allowed for sklearn pipelines\n",
    "class ModelWrapper(Pipeline, BaseEstimator):\n",
    "    def __init__(self, model: BaseEstimator):\n",
    "        super().__init__(steps=[])\n",
    "        self.model = model\n",
    "        self._is_fitted = False\n",
    "\n",
    "    def fit(self, x_train: pd.DataFrame, y_train: pd.Series = None, **kwargs):\n",
    "        self.model.fit(x_train, y_train)\n",
    "        self._is_fitted = True\n",
    "        return self\n",
    "\n",
    "    def predict(self, x: pd.DataFrame, **kwargs) -> pd.Series:\n",
    "        if not self._is_fitted:\n",
    "            raise ValueError(\"Model is not fitted yet. Call 'fit' before 'predict'.\")\n",
    "        return self.model.predict(x)\n",
    "\n",
    "# Generate some random data. Use your own data here\n",
    "def generate_random_data() -> pd.DataFrame:\n",
    "    data = {\n",
    "        \"a\": np.random.random(100),\n",
    "        \"b\": np.random.random(100),\n",
    "        \"c\": np.random.random(100),\n",
    "    }\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "\n",
    "# Train a dummy model\n",
    "def train_model(x: pd.DataFrame, y: pd.Series) -> ExtraTreesRegressor:\n",
    "    model = ExtraTreesRegressor()\n",
    "    model.fit(x, y)\n",
    "    return model\n",
    "\n",
    "\n",
    "def create_model_entity() -> ModelEntity:\n",
    "    # Generate random data\n",
    "    x = generate_random_data()\n",
    "    y = pd.Series(np.random.random(100))\n",
    "\n",
    "    # Train test split\n",
    "    x_train = x[:80]\n",
    "    y_train = y[:80]\n",
    "    x_test = x[80:]\n",
    "    y_test = y[80:]\n",
    "\n",
    "    model = train_model(x_train, y_train)\n",
    "    # Save model, training data and loss function to a ModelEntity. ModelEntity will cache the data to \"save_dir\"\n",
    "    model_entity = ModelEntity(\n",
    "        # Change the save_dir\n",
    "        path=\"save_dir\",\n",
    "        id=str(uuid4()),\n",
    "        pipeline=model,\n",
    "        train_x=x_train,\n",
    "        train_y=y_train,\n",
    "        test_x=x_test,\n",
    "        test_y=y_test,\n",
    "        loss=mean_absolute_error,\n",
    "        feature_list=[\"a\", \"b\", \"c\"],\n",
    "        # Add any other metadata you want to store\n",
    "        metadata={\"description\": \"Test model\"},\n",
    "    )\n",
    "\n",
    "    return model_entity\n",
    "\n",
    "# Create a dataset of ModelEntity objects\n",
    "dataset = ModelDataSet.from_iterable([create_model_entity() for _ in range(100)])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Set up a local Dask cluster\n",
    "client = LocalCluster(n_workers=1, threads_per_worker=1).get_client()"
   ],
   "id": "b3394bc6737353df",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "pipeline_factory = PipelineFactory(\n",
    "    dataset,\n",
    "    client,\n",
    "    # We want to retrain for consolidation and merge the models' data\n",
    "    RetrainingConsolidationStrategy(ModelWrapper(ExtraTreesRegressor()), PandasModelEntityDataMerger()),\n",
    ")\n",
    "# Create the embedding pipeline\n",
    "pipeline = PredictionLossSetPipelineBuilder(\n",
    "    pipeline_factory,\n",
    "    # We use 10 clusters. Change this to your needs\n",
    "    KMeans(n_clusters=10),\n",
    "    # The embedding dimension is 3. Change this to your needs\n",
    "    UniformSetSampler(3, TargetSelector(), Entropy()),\n",
    ").build_pipeline()\n",
    "pipeline.fit(dataset)"
   ],
   "id": "610e3d73b6a53567",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from modelforge.model_clustering.cluster.grid_search import ModelConsolidationScore\n",
    "\n",
    "# Calculate the score\n",
    "score: dict = pipeline.score(dataset)\n",
    "\n",
    "consolidation_score = ModelConsolidationScore.from_dict(score)\n",
    "\n",
    "print(\"Cluster loss\", consolidation_score.cluster_loss)\n",
    "clustering_df, embedding_df = consolidation_score.clustering.to_dataframe()\n",
    "embedding_df"
   ],
   "id": "e1aae2fd5670e0c4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "clustering_df",
   "id": "7772801ed6914891"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
